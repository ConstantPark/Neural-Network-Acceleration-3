## Neural Network Acceleration Study Season #3
This is a repository of the study "neural network acceleration". The goal of this study is to understand the acceleration of nerual networks on various devices. The topic of acceleration includes neural acceleration (such as inference on `CPU`, `GPU`, `NPU`, `ASIC`, `FPGA`,`NDP`), neural accelerator (`RTL`, `HLS`), and designing lightweight neural network (`Quantization`, `Pruning`, etc).Our materials are open to this github and youtube. This study is supported by Facebook community, "AI Robitcs Korea".

#### CPU/GPU, NPU, and distributed computing
- Fast acceleration of inference/training on general processor (CPU/GPU)
- Distributed computing for large training system
- Heterogeneous system architecture (HSA) device

#### ASIC and FPGA
- Low-power inference acceleration using RTL/HLS design
- High computing performance interfence/training accelerator

#### Lightweight neural network
- Quantization (Fixed-point, LUT, log, etc)
- Pruning (Fine/Course-grain, Channel-wise, etc)


## Paper List (16)
### CPU/GPU/NPU/DSP based Acceleration (?)

### Dedicated neural network accelerator (?)
	1. Integrating NVIDIA Deep Learning Accelerator (NVDLA) with RISC-V SoC on FireSim, EMC2, 2019


### Designing lightweight neural network (?)
	
## Presentation with Video
### Week1: Introduction (September 09, 2020)
**Integrating NVIDIA Deep Learning Accelerator (NVDLA) with RISC-V SoC on FireSim**

	Presenter: 
	PPT: 
	Video: 




## Contributors
**Main Contributor**: Constant Park (sonicstage12@naver.com)

**Presenters**: Constant Park (sonicstage12@naver.com), Jeongah-Shin (jeongah.arie@gmail.com), 김용우 (yongwoo.kim@smu.ac.kr), 강준구 (gjk6626@gmail.com), 이재민 (leejaymin@etri.re.kr), 금나연 (nayounkeum@gmail.com), 김영범 (eddy979793@gmail.com)
